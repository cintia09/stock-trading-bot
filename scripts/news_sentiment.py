#!/usr/bin/env python3
"""
æ–°é—»ä¸Žèˆ†æƒ…åˆ†æžæ¨¡å— - èŽ·å–è´¢ç»æ–°é—»å¹¶åˆ†æžæƒ…ç»ª
"""

import requests
import re
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict

BASE_DIR = Path(__file__).parent.parent
NEWS_DIR = BASE_DIR / "news"
NEWS_DIR.mkdir(exist_ok=True)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

# æƒ…ç»ªè¯å…¸
POSITIVE_WORDS = [
    "ä¸Šæ¶¨", "å¤§æ¶¨", "æ¶¨åœ", "é£™å‡", "æš´æ¶¨", "çªç ´", "æ–°é«˜", "åˆ©å¥½", "å¢žé•¿", "ç›ˆåˆ©",
    "è¶…é¢„æœŸ", "å¼ºåŠ¿", "åå¼¹", "å›žæš–", "çœ‹å¥½", "æŽ¨è", "ä¹°å…¥", "å¢žæŒ", "æå‡", "æ‰©å¼ ",
    "åˆ›æ–°é«˜", "æ”¾é‡", "ä¸»åŠ›", "èµ„é‡‘æµå…¥", "åŒ—å‘ä¹°å…¥", "æœºæž„åŠ ä»“", "ä¸šç»©å¤§å¢ž", "è®¢å•å¢žé•¿",
    "æ”¿ç­–æ”¯æŒ", "é‡å¤§çªç ´", "æŠ€æœ¯é¢†å…ˆ", "å¸‚åœºä»½é¢", "é¾™å¤´"
]

NEGATIVE_WORDS = [
    "ä¸‹è·Œ", "å¤§è·Œ", "è·Œåœ", "æš´è·Œ", "ç ´ä½", "æ–°ä½Ž", "åˆ©ç©º", "äºæŸ", "å‡æŒ", "æŠ›å”®",
    "ä¸šç»©ä¸‹æ»‘", "å¼±åŠ¿", "è·³æ°´", "å›žè°ƒ", "çœ‹ç©º", "å–å‡º", "å‡ä»“", "ä¸‹è°ƒ", "æ”¶ç¼©", "èŽç¼©",
    "èµ„é‡‘æµå‡º", "åŒ—å‘å–å‡º", "æœºæž„å‡æŒ", "ä¸šç»©çˆ†é›·", "è®¢å•ä¸‹æ»‘", "ç›‘ç®¡å¤„ç½š", "è¯‰è®¼", "è¿è§„"
]

SECTOR_KEYWORDS = {
    "è´µé‡‘å±ž": ["é»„é‡‘", "ç™½é“¶", "è´µé‡‘å±ž", "é‡‘ä»·", "é¿é™©"],
    "æ–°èƒ½æºè½¦": ["æ–°èƒ½æº", "ç”µåŠ¨è½¦", "é”‚ç”µ", "å……ç”µæ¡©", "æ¯”äºšè¿ª", "ç‰¹æ–¯æ‹‰"],
    "AI": ["äººå·¥æ™ºèƒ½", "AI", "å¤§æ¨¡åž‹", "ç®—åŠ›", "èŠ¯ç‰‡", "GPU", "è‹±ä¼Ÿè¾¾"],
    "æ¶ˆè´¹": ["æ¶ˆè´¹", "ç™½é…’", "èŒ…å°", "é›¶å”®", "æ—…æ¸¸", "å…ç¨Ž"],
    "é“¶è¡Œ": ["é“¶è¡Œ", "é‡‘èž", "åˆ©çŽ‡", "é™æ¯", "å­˜æ¬¾"],
    "å…‰ä¼": ["å…‰ä¼", "å¤ªé˜³èƒ½", "ç¡…æ–™", "ç»„ä»¶"],
    "åŒ»è¯": ["åŒ»è¯", "åŒ»ç–—", "åˆ›æ–°è¯", "é›†é‡‡"],
    "æˆ¿åœ°äº§": ["æˆ¿åœ°äº§", "åœ°äº§", "æ¥¼å¸‚", "æˆ¿ä»·", "ä½æˆ¿"]
}

def fetch_eastmoney_news(limit: int = 50) -> List[Dict]:
    """èŽ·å–ä¸œæ–¹è´¢å¯Œè´¢ç»æ–°é—»"""
    news_list = []
    
    # ä¸œæ–¹è´¢å¯Œ7x24å¿«è®¯
    url = "https://np-listapi.eastmoney.com/comm/web/getFastNewsList"
    params = {
        "client": "web",
        "biz": "web_724",
        "fastColumn": "",
        "sortEnd": "",
        "pageSize": limit,
        "req_trace": str(int(datetime.now().timestamp() * 1000))
    }
    
    try:
        resp = requests.get(url, params=params, headers=HEADERS, timeout=10)
        data = resp.json()
        
        if data.get("data") and data["data"].get("fastNewsList"):
            for item in data["data"]["fastNewsList"]:
                news_list.append({
                    "title": item.get("title", ""),
                    "content": item.get("digest", ""),
                    "time": item.get("showTime", ""),
                    "source": "ä¸œæ–¹è´¢å¯Œ",
                    "url": f"https://finance.eastmoney.com/a/{item.get('code', '')}.html"
                })
    except Exception as e:
        print(f"ä¸œæ–¹è´¢å¯Œæ–°é—»èŽ·å–å¤±è´¥: {e}")
    
    return news_list

def fetch_sina_news(limit: int = 30) -> List[Dict]:
    """èŽ·å–æ–°æµªè´¢ç»æ–°é—»"""
    news_list = []
    
    url = "https://feed.mix.sina.com.cn/api/roll/get"
    params = {
        "pageid": 153,
        "lid": 2516,
        "k": "",
        "num": limit,
        "page": 1
    }
    
    try:
        resp = requests.get(url, params=params, headers=HEADERS, timeout=10)
        data = resp.json()
        
        if data.get("result") and data["result"].get("data"):
            for item in data["result"]["data"]:
                news_list.append({
                    "title": item.get("title", ""),
                    "content": item.get("intro", ""),
                    "time": item.get("ctime", ""),
                    "source": "æ–°æµªè´¢ç»",
                    "url": item.get("url", "")
                })
    except Exception as e:
        print(f"æ–°æµªè´¢ç»æ–°é—»èŽ·å–å¤±è´¥: {e}")
    
    return news_list

def analyze_sentiment(text: str) -> Dict:
    """åˆ†æžæ–‡æœ¬æƒ…ç»ª"""
    if not text:
        return {"score": 0, "label": "neutral", "positive": [], "negative": []}
    
    positive_found = []
    negative_found = []
    
    for word in POSITIVE_WORDS:
        if word in text:
            positive_found.append(word)
    
    for word in NEGATIVE_WORDS:
        if word in text:
            negative_found.append(word)
    
    score = len(positive_found) - len(negative_found)
    
    if score > 2:
        label = "very_positive"
    elif score > 0:
        label = "positive"
    elif score < -2:
        label = "very_negative"
    elif score < 0:
        label = "negative"
    else:
        label = "neutral"
    
    return {
        "score": score,
        "label": label,
        "positive": list(set(positive_found)),
        "negative": list(set(negative_found))
    }

def extract_stock_mentions(text: str, stock_dict: Dict[str, str]) -> List[str]:
    """æå–æ–°é—»ä¸­æåˆ°çš„è‚¡ç¥¨"""
    mentioned = []
    for code, name in stock_dict.items():
        if name in text or code in text:
            mentioned.append(code)
    return mentioned

def identify_sectors(text: str) -> List[str]:
    """è¯†åˆ«æ–°é—»æ¶‰åŠçš„æ¿å—"""
    sectors = []
    for sector, keywords in SECTOR_KEYWORDS.items():
        for kw in keywords:
            if kw in text:
                sectors.append(sector)
                break
    return sectors

def analyze_news_batch(news_list: List[Dict], stock_dict: Dict[str, str] = None) -> Dict:
    """æ‰¹é‡åˆ†æžæ–°é—»"""
    if stock_dict is None:
        stock_dict = {}
    
    overall_sentiment = 0
    sector_sentiment = {sector: {"count": 0, "score": 0} for sector in SECTOR_KEYWORDS}
    stock_mentions = {}
    important_news = []
    
    for news in news_list:
        full_text = news.get("title", "") + " " + news.get("content", "")
        
        # æƒ…ç»ªåˆ†æž
        sentiment = analyze_sentiment(full_text)
        overall_sentiment += sentiment["score"]
        
        # æ¿å—è¯†åˆ«
        sectors = identify_sectors(full_text)
        for sector in sectors:
            sector_sentiment[sector]["count"] += 1
            sector_sentiment[sector]["score"] += sentiment["score"]
        
        # è‚¡ç¥¨æåŠ
        if stock_dict:
            mentioned = extract_stock_mentions(full_text, stock_dict)
            for code in mentioned:
                if code not in stock_mentions:
                    stock_mentions[code] = {"count": 0, "sentiment": 0, "news": []}
                stock_mentions[code]["count"] += 1
                stock_mentions[code]["sentiment"] += sentiment["score"]
                stock_mentions[code]["news"].append(news["title"])
        
        # é‡è¦æ–°é—»(é«˜æƒ…ç»ªåˆ†)
        if abs(sentiment["score"]) >= 2:
            important_news.append({
                "title": news["title"],
                "sentiment": sentiment,
                "sectors": sectors,
                "time": news.get("time", "")
            })
    
    # è®¡ç®—æ¿å—æƒ…ç»ªå‡å€¼
    for sector in sector_sentiment:
        if sector_sentiment[sector]["count"] > 0:
            sector_sentiment[sector]["avg_score"] = round(
                sector_sentiment[sector]["score"] / sector_sentiment[sector]["count"], 2
            )
        else:
            sector_sentiment[sector]["avg_score"] = 0
    
    # æŽ’åºæ¿å—
    hot_sectors = sorted(
        [(s, d["count"], d["avg_score"]) for s, d in sector_sentiment.items() if d["count"] > 0],
        key=lambda x: x[1],
        reverse=True
    )
    
    return {
        "overall_sentiment": overall_sentiment,
        "overall_label": "positive" if overall_sentiment > 5 else ("negative" if overall_sentiment < -5 else "neutral"),
        "hot_sectors": hot_sectors,
        "stock_mentions": stock_mentions,
        "important_news": important_news[:10],
        "total_news": len(news_list),
        "analyzed_at": datetime.now().isoformat()
    }

def get_market_sentiment() -> Dict:
    """èŽ·å–ç»¼åˆå¸‚åœºæƒ…ç»ª"""
    # èŽ·å–æ–°é—»
    em_news = fetch_eastmoney_news(50)
    sina_news = fetch_sina_news(30)
    all_news = em_news + sina_news
    
    # åˆ†æž
    analysis = analyze_news_batch(all_news)
    
    # ä¿å­˜
    save_path = NEWS_DIR / f"sentiment_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    return analysis

if __name__ == "__main__":
    print("èŽ·å–è´¢ç»æ–°é—»...")
    sentiment = get_market_sentiment()
    
    print(f"\næ€»ä½“æƒ…ç»ª: {sentiment['overall_label']} (å¾—åˆ†: {sentiment['overall_sentiment']})")
    print(f"åˆ†æžæ–°é—»æ•°: {sentiment['total_news']}")
    
    print("\nçƒ­é—¨æ¿å—:")
    for sector, count, score in sentiment["hot_sectors"][:5]:
        emoji = "ðŸŸ¢" if score > 0 else ("ðŸ”´" if score < 0 else "âšª")
        print(f"  {emoji} {sector}: æåŠ{count}æ¬¡, æƒ…ç»ª{score:+.1f}")
    
    print("\né‡è¦æ–°é—»:")
    for news in sentiment["important_news"][:5]:
        emoji = "ðŸ“ˆ" if news["sentiment"]["score"] > 0 else "ðŸ“‰"
        print(f"  {emoji} {news['title'][:40]}...")
